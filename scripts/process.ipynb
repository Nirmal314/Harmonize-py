{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '../dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and dataset information\n",
    "print(\"Initial Data:\")\n",
    "print(data.head())\n",
    "print(\"\\nData Information:\")\n",
    "data.info()\n",
    "\n",
    "# Step 1: Drop the 'Unnamed: 0' column as it's redundant\n",
    "data_cleaned = data.drop(columns=['Unnamed: 0'])\n",
    "print(\"\\nData after dropping 'Unnamed: 0':\")\n",
    "\n",
    "# Step 2: Handle missing values by dropping rows with missing values in critical columns\n",
    "data_cleaned = data_cleaned.dropna(subset=['artists', 'album_name', 'track_name'])\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(data_cleaned.isnull().sum())\n",
    "\n",
    "# Step 3: Remove duplicates based on 'track_id'\n",
    "data_cleaned = data_cleaned.drop_duplicates(subset=['track_id'])\n",
    "print(\"\\nShape of the data after removing duplicates:\", data_cleaned.shape)\n",
    "\n",
    "# Final Cleaned Data Overview\n",
    "print(\"\\nData Overview:\")\n",
    "print(data_cleaned.describe())\n",
    "\n",
    "output_path = '../cleaned_dataset.csv'\n",
    "data_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved as {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv('../cleaned_dataset.csv')\n",
    "\n",
    "# Define features for clustering\n",
    "features_for_clustering = ['danceability', 'energy', 'valence', 'tempo', 'key', 'loudness', 'mode', \n",
    "                           'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'time_signature']\n",
    "\n",
    "# Fit k-means clustering for all features\n",
    "kmeans_all = KMeans(n_clusters=4, random_state=42).fit(data[features_for_clustering])\n",
    "data['cluster'] = kmeans_all.labels_\n",
    "\n",
    "# Define labels based on clusters\n",
    "data['is_happy'] = data['cluster'].apply(lambda x: 1 if x == 0 else 0)  # Example mapping\n",
    "data['is_sad'] = data['cluster'].apply(lambda x: 1 if x == 1 else 0)\n",
    "data['is_fast'] = data['cluster'].apply(lambda x: 1 if x == 2 else 0)\n",
    "data['is_slow'] = data['cluster'].apply(lambda x: 1 if x == 3 else 0)\n",
    "\n",
    "# Plot the distribution of labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=data, x='is_happy', label='Happy')\n",
    "sns.countplot(data=data, x='is_sad', label='Sad')\n",
    "sns.countplot(data=data, x='is_fast', label='Fast')\n",
    "sns.countplot(data=data, x='is_slow', label='Slow')\n",
    "\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['Happy', 'Sad', 'Fast', 'Slow'])\n",
    "plt.show()\n",
    "\n",
    "# Plot feature distributions for all features\n",
    "data[features_for_clustering].hist(figsize=(14, 12))\n",
    "plt.suptitle('Feature Distributions')\n",
    "plt.show()\n",
    "\n",
    "# Define features and targets\n",
    "X = data[features_for_clustering]\n",
    "y = data[['is_happy', 'is_sad', 'is_fast', 'is_slow']]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Multi-Output Classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "multi_target_classifier = MultiOutputClassifier(classifier, n_jobs=-1)\n",
    "multi_target_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = multi_target_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Happy', 'Sad', 'Fast', 'Slow']))\n",
    "\n",
    "# Plot the distribution of new labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=data, x='is_happy')\n",
    "plt.title('Distribution of Happy Labels')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=data, x='is_sad')\n",
    "plt.title('Distribution of Sad Labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=data, x='is_fast')\n",
    "plt.title('Distribution of Fast Labels')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=data, x='is_slow')\n",
    "plt.title('Distribution of Slow Labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, '../pkl/scaler.pkl')\n",
    "\n",
    "# Save the multi-output classifier\n",
    "joblib.dump(multi_target_classifier, '../pkl/multi_target_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import joblib\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client_id = os.getenv('CLIENT_ID')\n",
    "client_secret = os.getenv('CLIENT_SECRET')\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyOAuth(client_id=client_id,\n",
    "                                               client_secret=client_secret,\n",
    "                                               redirect_uri='https://google.com'))\n",
    "\n",
    "playlist_id = '2uORYX3pVmRBUJe8uXrK8H'\n",
    "results = sp.playlist_tracks(playlist_id)\n",
    "tracks = results['items']\n",
    "\n",
    "data = []\n",
    "urls = []\n",
    "\n",
    "for track in tracks:\n",
    "    song_id = track['track']['id']\n",
    "    features = sp.audio_features(song_id)[0]\n",
    "    if features is not None:\n",
    "        data.append({\n",
    "            'danceability': features['danceability'],\n",
    "            'energy': features['energy'],\n",
    "            'valence': features['valence'],\n",
    "            'tempo': features['tempo'],\n",
    "            'key': features['key'],\n",
    "            'loudness': features['loudness'],\n",
    "            'mode': features['mode'],\n",
    "            'speechiness': features['speechiness'],\n",
    "            'acousticness': features['acousticness'],\n",
    "            'instrumentalness': features['instrumentalness'],\n",
    "            'liveness': features['liveness'],\n",
    "            'time_signature': features['time_signature']\n",
    "        })\n",
    "        urls.append(track['track']['external_urls']['spotify'])\n",
    "\n",
    "features_df = pd.DataFrame(data)\n",
    "\n",
    "# Load the scaler and classifier\n",
    "scaler = joblib.load('../pkl/scaler.pkl')\n",
    "multi_target_classifier = joblib.load('../pkl/multi_target_classifier.pkl')\n",
    "\n",
    "# Ensure the features used match the training set\n",
    "required_features = ['danceability', 'energy', 'valence', 'tempo', 'key', 'loudness', 'mode',\n",
    "                     'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'time_signature']\n",
    "features_df = features_df[required_features]\n",
    "\n",
    "# Scale the features\n",
    "features_scaled = scaler.transform(features_df)\n",
    "predicted_categories = multi_target_classifier.predict(features_scaled)\n",
    "\n",
    "categories = ['Happy', 'Sad', 'Fast', 'Slow']\n",
    "results = []\n",
    "\n",
    "for i, track in enumerate(tracks):\n",
    "    happy_sad = [categories[j] for j, val in enumerate(predicted_categories[i]) if j < 2 and val == 1]\n",
    "    fast_slow = [categories[j] for j, val in enumerate(predicted_categories[i]) if j >= 2 and val == 1]\n",
    "    \n",
    "    # Ensure exactly one category from each group\n",
    "    if not happy_sad:\n",
    "        happy_sad = ['Happy'] if 'Happy' in categories[:2] else ['Sad']\n",
    "    if not fast_slow:\n",
    "        fast_slow = ['Fast'] if 'Fast' in categories[2:] else ['Slow']\n",
    "    \n",
    "    # Combine results\n",
    "    combined_labels = happy_sad + fast_slow\n",
    "    \n",
    "    results.append({\n",
    "        'track': track['track']['name'],\n",
    "        'artist': track['track']['artists'][0]['name'],\n",
    "        'categories': ', '.join(combined_labels),\n",
    "        'url': urls[i]\n",
    "    })\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Track: {result['track']}\\nArtist: {result['artist']}\\nCategories: {result['categories']}\\nURL: {result['url']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
